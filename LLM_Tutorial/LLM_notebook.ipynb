{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bae96f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- To be able to import from other modules here ---\n",
    "import sys; import os; sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from utils.save_response import writeMd\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, create_model, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83983c",
   "metadata": {},
   "source": [
    "### Loading the environment variables\n",
    "We can use the dotenv library to load all the environment variables written in a local.env file.\n",
    " --> We only have the `GEMINI_API_KEY` variable ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c578c761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = \"../local.env\"\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba1ff",
   "metadata": {},
   "source": [
    "### Google genai\n",
    "You can refer to the [official documentation](https://ai.google.dev/gemini-api/docs) for more details on how to use the Google Gemini API with Python.\n",
    "\n",
    "First, have a look at the [Quickstart](https://ai.google.dev/gemini-api/docs/quickstart) on the left panel. Note that you don't need to install anything (ignore pip install instructions), as we have already installed everything for you. \n",
    "\n",
    "Try out the code with different prompt contents and different models to see if your environment works!\n",
    "\n",
    "![fig1](figs/gemini_quickstart.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2738a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158f579",
   "metadata": {},
   "source": [
    "### TEXT GENERATION WITH THINKING\n",
    "Here, you can see all the different knobs that you can control when calling a model for text generation.\n",
    "\n",
    "You can choose whether or not to `include_thoughts` by setting it to True/False, or change the maximum number of thinking tokens as the `thinking_budget`.\n",
    "\n",
    "You can change how probabilistic/deterministic the model samples tokens at its output: `temperature` of 0.0 means complete deterministic here ...\n",
    "\n",
    "You can also control the maximum number of output tokens ...\n",
    "\n",
    "For a more comprehensive list and explanation of variables, you can read [Generating content](https://ai.google.dev/api/generate-content#v1beta.GenerationConfig) page.\n",
    "\n",
    "In the following cell, you see an example of using `gemini-2.5-flash` as a math assistant.\n",
    "\n",
    "We have provided a *writeMd* function for you (which is in the *utils* module) that saves outputs (along with gemini's thinking summary) in a markdown file.\n",
    "\n",
    "The *writeMd* saves the output in the `no_backup/markdown_files` folder, you can open the generated file and use `Ctrl+Shift+V` to show it in preview mode ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238f4943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "\n",
      "file:///users/micas/mahmadza/Documents/THESIS/CAD/no_backup/markdown_files/output.md\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=10_000, include_thoughts=True), # Disables thinking\n",
    "        temperature=0.0,  # Sets deterministic output\n",
    "        max_output_tokens=20_000, # Sets maximum output tokens\n",
    "        seed=42,  # Sets a seed for reproducibility\n",
    "        candidate_count=1,  # Number of response candidates to generate\n",
    "        system_instruction=\"You're a math teacher and assistant that provides guided answers to math questions.\",\n",
    "    ),\n",
    "    contents=\"What is the Euler's formula? How did he find it out?\", # The prompt to generate content for\n",
    ")\n",
    "\n",
    "writeMd(response, filename=\"output.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b8fc2",
   "metadata": {},
   "source": [
    "### STRUCTURED OUTPUT\n",
    "\n",
    "We can use the **pydantic** library to define an output style (or *response schema*) that the LLM should stick to:\n",
    "\n",
    "1. Using the `BaseModel`:\n",
    "    create a class that inherits from *BaseModel* and write its fields with their corresponding data types and explanation\n",
    "\n",
    "2. Using the `create_model`:\n",
    "    create a dictionary of the field names and their corresponding data types and use the *create_model* to create a data class from it\n",
    "\n",
    "Read the **Structured output** tab in the left panel of Gemini api documentation for more information. [click here](https://ai.google.dev/gemini-api/docs/structured-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "320162c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"city\":\"Tokyo\",\"population\":14000000,\"area\":2194}\n"
     ]
    }
   ],
   "source": [
    "# 1.\n",
    "class CityInfo_1(BaseModel):\n",
    "    city: str = Field(..., description=\"Name of the city\")\n",
    "    population: int = Field(..., description=\"Population of the city\")\n",
    "    area: float = Field(..., description=\"Area of the city in square kilometers\")\n",
    "\n",
    "# 2.\n",
    "fields_dict = {'city': str, 'population': int, 'area': float}\n",
    "CityInfo_2 = create_model(\n",
    "    'CityInfo_2',\n",
    "    **{field_name: (data_type, Field(...)) for field_name, data_type in fields_dict.items()}\n",
    ")\n",
    "\n",
    "\n",
    "# The Field can also include other metadata like ge/le (greater or equal, less or equal) ...\n",
    "\n",
    "# 3.\n",
    "class CityInfo_3(BaseModel):\n",
    "    city: str = Field(..., description=\"Name of the city\")\n",
    "    population: int = Field(..., description=\"Population of the city\", ge=10_000, le=50_000_000)\n",
    "    area: float = Field(..., description=\"Area of the city in square kilometers\", ge=0.5, le=500_000)\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Provide information about city of Tokyo including its population and surface area.\",\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_schema\": CityInfo_3\n",
    "    },\n",
    ")\n",
    "print(response.text)\n",
    "response_data = response.parsed\n",
    "response_dict = dict(response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15864982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class '__main__.CityInfo_3'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response.text))  # str\n",
    "print(type(response_data))  # CityInfo\n",
    "print(type(response_dict))  # dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237a35f",
   "metadata": {},
   "source": [
    "### FUNCTION CALLING: Equipt the LLM with a Calculator!\n",
    "\n",
    "You can write your own python function and write a decleration for it (see below cell) and give it to the LLM as a **tool**.\n",
    "\n",
    "The LLM will decide to call it when it sees necessary.\n",
    "\n",
    "The decleration of the function should include all its argument descriptions.\n",
    "\n",
    "When the LLM decides to call the function, it will provide the name of the function as well as the arguments to pass to the function. **It will not actually run the function by itself**.\n",
    "\n",
    "Our python code should be written in a way that detects when the LLM contains a function calling part, runs the function, and gives back the output of the function to the LLM as a second call.\n",
    "\n",
    "The reslut of the function should be appended to the contents (ongoing conversation from previous call) so that the LLM knows this second call is the continuation of the first call and contains the result of the function run. \n",
    "\n",
    "For more information, please visit the [Function Calling](https://ai.google.dev/gemini-api/docs/function-calling) tab in the gemini api documentation.\n",
    "\n",
    "In the below example, our function (external tool) is a calculator! However, in your project, the RL-Sizer will be the external optimizer tool.\n",
    "\n",
    "Your system prompt to the LLM can explain when to call the tools. This is not necessary for easy tools like a calculator;\n",
    "    \n",
    " but for sophisticated tools like the RL sizer, you can indicate when it should be used, for how many runs, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that the model can call to calculate basic arithmetic operations\n",
    "calculator_declaration = {\n",
    "    \"name\": \"calculator\",\n",
    "    \"description\": \"A simple calculator to perform basic arithmetic operations.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The arithmetic operation to perform: add, subtract, multiply, divide, power, \\\n",
    "                                                                     exponent, square_root, log10, ln, to_degrees, \\\n",
    "                                                                     to_radians, sine, cosine, tangent, arcsine, \\\n",
    "                                                                     arccosine, arctangent.\",\n",
    "            },\n",
    "            \"num1\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The first number.\",\n",
    "            },\n",
    "            \"num2\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The second number (if needed).\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"operation\", \"num1\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "import math\n",
    "# This is the actual function that would be called based on the model's suggestion\n",
    "def calculator(operation: str, num1: float, num2: float = None) -> float:\n",
    "    if operation == \"add\":\n",
    "        return num1 + num2\n",
    "    elif operation == \"subtract\":\n",
    "        return num1 - num2\n",
    "    elif operation == \"multiply\":\n",
    "        return num1 * num2\n",
    "    elif operation == \"divide\":\n",
    "        if num2 != 0:\n",
    "            return num1 / num2\n",
    "        else:\n",
    "            return \"Error: Division by zero\"\n",
    "    elif operation == \"power\":\n",
    "        return num1 ** num2\n",
    "    elif operation == \"exponent\":\n",
    "        return math.exp(num1)\n",
    "    elif operation == \"square_root\":\n",
    "        return math.sqrt(num1)\n",
    "    elif operation == \"log10\":\n",
    "        return math.log10(num1)\n",
    "    elif operation == \"ln\":\n",
    "        return math.log(num1)\n",
    "    elif operation == \"to_degrees\":\n",
    "        return math.degrees(num1)\n",
    "    elif operation == \"to_radians\":\n",
    "        return math.radians(num1)\n",
    "    elif operation == \"sine\":\n",
    "        return math.sin(num1)\n",
    "    elif operation == \"cosine\":\n",
    "        return math.cos(num1)\n",
    "    elif operation == \"tangent\":\n",
    "        if num1 % (math.pi/2) == 0 and (num1 / (math.pi/2)) % 2 != 0:\n",
    "            return \"Error: Tangent undefined at this angle\"\n",
    "        return math.tan(num1)\n",
    "    elif operation == \"arcsine\":\n",
    "        return math.asin(num1)\n",
    "    elif operation == \"arccosine\":\n",
    "        return math.acos(num1)\n",
    "    elif operation == \"arctangent\":\n",
    "        return math.atan(num1)\n",
    "    else:\n",
    "        return \"Error: Unknown operation\"\n",
    "\n",
    "tools = types.Tool(function_declarations=[calculator_declaration])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0625513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "id=None args={'num1': 4.71238898025, 'operation': 'sine'} name='calculator'\n",
      "====================================================================================================\n",
      "Function execution result:\n",
      " -1.0\n",
      "====================================================================================================\n",
      "The sine of 3Ï€/2 is -1.\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"Calculate the multiplication of 15.5 and 24.3.\"\n",
    "prompt_2 = \"What is the square root of 256?\"\n",
    "prompt_3 = \"Compute the log10 of 1000.\"\n",
    "prompt_3_= \"give me the natural logarithm of 1000.\"\n",
    "prompt_4 = \"Add 100 and 250.\"\n",
    "prompt_5 = \"Divide 500 by 0.\"\n",
    "prompt_6 = \"Raise 2 to the power of 8.\"\n",
    "prompt_7 = \"What is the sine of 3*pi/2?\"\n",
    "prompt_8 = \"What is the tangent of 90 degrees?\"\n",
    "\n",
    "# Define user prompt\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=prompt_7)]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(response.candidates[0].content.parts[0].function_call)\n",
    "\n",
    "# Extract tool call details, it may not be in the first part.\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if tool_call.name == \"calculator\":\n",
    "    result = calculator(**tool_call.args)\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Function execution result:\\n {result}\")\n",
    "\n",
    "# Create a function response part\n",
    "function_response_part = types.Part.from_function_response(\n",
    "    name=tool_call.name,\n",
    "    response={\"result\": result},\n",
    ")\n",
    "\n",
    "# Append function call and result of the function execution to contents\n",
    "contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
    "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "\n",
    "client = genai.Client()\n",
    "final_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=config,\n",
    "    contents=contents,\n",
    ")\n",
    "print(\"=\"*100)\n",
    "print(final_response.text)\n",
    "\n",
    "\n",
    "if final_response.candidates[0].content.parts[0].function_call:\n",
    "    print(\"another function call detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b83b44",
   "metadata": {},
   "source": [
    "### ReAct agent: Multiple calls to the Tool\n",
    "\n",
    "#### ReAct --> Reason & Act\n",
    "\n",
    "As you can see in the above example, when we use prompt 8, the LLM tries to first call the calculator for converting degrees to radians.\n",
    "\n",
    "After that, it will try to use the calculator for a second time for the tangent of the converted angle ...\n",
    "\n",
    "Therefore, our code can have a loop: while the output of the LLM still contains a call to a function, our code should handle the function calling and appending the result to the contents ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f15e222b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user prompt: 79247 * 2343 - 44356/tangent(44 degrees)\n",
      "====================================================================================================\n",
      "Function execution result:\n",
      " 0.017704699278685777\n",
      "====================================================================================================\n",
      "Function execution result:\n",
      " 2505323.5472572544\n",
      "====================================================================================================\n",
      "Function execution result:\n",
      " 185675721\n",
      "====================================================================================================\n",
      "Function execution result:\n",
      " 183170397.45274276\n",
      "====================================================================================================\n",
      "# Answer Text\n",
      "The answer is 183170397.45274276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter your own prompt: \")\n",
    "if prompt.strip() == \"\":\n",
    "    prompt = \"12 ^ (tangent(135 degrees) - exp(2))\"\n",
    "\n",
    "print (\"user prompt:\", prompt)\n",
    "\n",
    "# Define user prompt\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=prompt)]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Extract tool call details, it may not be in the first part.\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "while tool_call:\n",
    "    if tool_call.name == \"calculator\":\n",
    "        result = calculator(**tool_call.args)\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Function execution result:\\n {result}\")\n",
    "    # Create a function response part\n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name=tool_call.name,\n",
    "        response={\"result\": result},\n",
    "    )\n",
    "\n",
    "    # Append function call and result of the function execution to contents\n",
    "    contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
    "    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "\n",
    "    client = genai.Client()\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=config,\n",
    "        contents=contents,\n",
    "    )\n",
    "    tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "md_string = \"\"\n",
    "# monitoring the thinking summary and the final answer\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if not part.text:\n",
    "        continue\n",
    "    if part.thought:\n",
    "        md_string += f\"# Thought Summary\\n{part.text}\\n\"\n",
    "    else:\n",
    "        md_string += f\"# Answer Text\\n{part.text}\\n\"\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(md_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fb061",
   "metadata": {},
   "source": [
    "What you have above is a POWERFUL calculator that can handle any complex type of calculation: be it written in sentence form or mathematical expression form ðŸ˜‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cad_env (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
